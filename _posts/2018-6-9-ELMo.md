---
layout: post
title: 一种新的word embedding --Deep contextualized word representations (ELMo)
date: 2018-06-09
tags: Paper
---

### Introduction

之前的glove以及word2vec的word embedding在nlp任务中都取得了最好的效果, 现在几乎没有一个NLP的任务中不加word embedding.

我们常用的获取embedding方法都是通过训练language model, 将language model中预测的hidden state做为word的表示, 给定N个tokens的序列$$(t_1, t_2,...,t_n)$$, 前向language model就是通过前k-1个输入序列$$(t_1, t_2, ...,t_k)$$的hidden表示, 预测第k个位置的token, 反向的language model就是给定后面的序列, 预测之前的, 然后将language model的第k个位置的hidden输出做为word embedding. 

之前的做法的缺点是对于每一个单词都有唯一的一个embedding表示, 而对于多义词显然这种做法不符合直觉, 而单词的意思又和上下文相关, ELMo的做法是我们只预训练language model, 而word embedding是通过输入的句子实时输出的, 这样单词的意思就是上下文相关的了, 这样就很大程度上缓解了歧义的发生.

### ELMo: Embeddings from Language Models

ELMo用到上文提到的双向的language model, 给定N个tokens (t1, t2,...,tN),  language model通过给定前面的k-1个位置的token序列计算第k个token的出现的概率:

$$p(t_1, t_2, ..., t_N) = \prod_{k=1}^N p(t_k|t_1, t_2, ..., t_{k-1})$$

后向的计算方法与前向相似:

$$p(t_1, t_2, ..., t_N) = \prod_{k=1}^N p(t_k\vert t_{k+1}, t_{k+2}, ..., t_{N})$$

biLM训练过程中的目标就是最大化:

$$\sum^N_{k=1}(\log p(t_k| t_1, ...,t_{k-1};\Theta _x, \overrightarrow{\Theta}_{LSTM}, \Theta _s) + \log p(t_k\vert t_{k+1}, ...,t_{N}; \Theta _x, \overleftarrow{\Theta}_{LSTM}, \Theta _s))$$

ELMo对于每个token $$t_k$$, 通过一个L层的biLM计算出2L+1个表示:

$$R_k = \{x_k^{LM}, \overrightarrow{h}_{k,j}^{LM}, \overleftarrow{h}_{k, j}^{LM} \vert j=1, ..., L\} = \{h_{k,j}^{LM} \vert j=0,..., L\}$$

其中$$h_{k,0}^{LM}$$ 是对token进行直接编码的结果(这里是字符通过CNN编码), $$h_{k,j}^{LM} = [\overrightarrow{h}_{k,j}^{LM}; \overleftarrow{h}_{k, j}^{LM}]$$ 是每个biLSTM层输出的结果.

应用中将ELMo中所有层的输出R压缩为单个向量, $$ELMo_k = E(R_k;\Theta _\epsilon)$$, 最简单的压缩方法是取最上层的结果做为token的表示: $$E(R_k) = h_{k,L}^{LM}$$, 更通用的做法是通过一些参数来联合所有层的信息:

$$ELMo_k^{task} = E(R_k;\Theta ^{task}) = \gamma ^{task} \sum _{j=0}^L s_j^{task}h_{k,j}^{LM}$$

其中$$s_j$$是一个softmax出来的结果, $$\gamma$$是一个任务相关的scale参数, 我试了平均每个层的信息和学出来$$s_j$$发现学习出来的效果会好很多.

文章中提到的Pre-trained的language model是用了两层的biLM, 对token进行上下文无关的编码是通过CNN对字符进行编码, 然后将三层的输出scale到1024维, 最后对每个token输出3个1024维的向量表示.

### 通过AllenNLP使用ELMo

与训练的ELMo已经放出, pytorch用户可以通过AlenNLP使用, 预训的Tensorflow版本也在 [TF版](https://github.com/allenai/bilm-tf) 中放出, 这里介绍一下通过AllenNLP包用ELMo的方法.




