---
layout: post
title: NAACL2018 一种新的embedding方法--原理与实验 Deep contextualized word representations (ELMo)
date: 2018-06-09
tags: Paper
---

### Introduction

之前的glove以及word2vec的word embedding在nlp任务中都取得了最好的效果, 现在几乎没有一个NLP的任务中不加word embedding.

我们常用的获取embedding方法都是通过训练language model, 将language model中预测的hidden state做为word的表示, 给定N个tokens的序列$$(t_1, t_2,...,t_n)$$, 前向language model就是通过前k-1个输入序列$$(t_1, t_2, ...,t_k)$$的hidden表示, 预测第k个位置的token, 反向的language model就是给定后面的序列, 预测之前的, 然后将language model的第k个位置的hidden输出做为word embedding. 

之前的做法的缺点是对于每一个单词都有唯一的一个embedding表示, 而对于多义词显然这种做法不符合直觉, 而单词的意思又和上下文相关, ELMo的做法是我们只预训练language model, 而word embedding是通过输入的句子实时输出的, 这样单词的意思就是上下文相关的了, 这样就很大程度上缓解了歧义的发生.

### ELMo: Embeddings from Language Models

ELMo用到上文提到的双向的language model, 给定N个tokens (t1, t2,...,tN),  language model通过给定前面的k-1个位置的token序列计算第k个token的出现的概率:

$$p(t_1, t_2, ..., t_N) = \prod_{k=1}^N p(t_k|t_1, t_2, ..., t_{k-1})$$

后向的计算方法与前向相似:

$$p(t_1, t_2, ..., t_N) = \prod_{k=1}^N p(t_k\vert t_{k+1}, t_{k+2}, ..., t_{N})$$

biLM训练过程中的目标就是最大化:

$$\sum^N_{k=1}(\log p(t_k| t_1, ...,t_{k-1};\Theta _x, \overrightarrow{\Theta}_{LSTM}, \Theta _s) + \log p(t_k\vert t_{k+1}, ...,t_{N}; \Theta _x, \overleftarrow{\Theta}_{LSTM}, \Theta _s))$$

ELMo对于每个token $$t_k$$, 通过一个L层的biLM计算出2L+1个表示:

$$R_k = \{x_k^{LM}, \overrightarrow{h}_{k,j}^{LM}, \overleftarrow{h}_{k, j}^{LM} \vert j=1, ..., L\} = \{h_{k,j}^{LM} \vert j=0,..., L\}$$

其中$$h_{k,0}^{LM}$$ 是对token进行直接编码的结果(这里是字符通过CNN编码), $$h_{k,j}^{LM} = [\overrightarrow{h}_{k,j}^{LM}; \overleftarrow{h}_{k, j}^{LM}]$$ 是每个biLSTM层输出的结果. 在实验中还发现不同层的biLM的输出的token表示对于不同的任务效果不同.

应用中将ELMo中所有层的输出R压缩为单个向量, $$ELMo_k = E(R_k;\Theta _\epsilon)$$, 最简单的压缩方法是取最上层的结果做为token的表示: $$E(R_k) = h_{k,L}^{LM}$$, 更通用的做法是通过一些参数来联合所有层的信息:

$$ELMo_k^{task} = E(R_k;\Theta ^{task}) = \gamma ^{task} \sum _{j=0}^L s_j^{task}h_{k,j}^{LM}$$

其中$$s_j$$是一个softmax出来的结果, $$\gamma$$是一个任务相关的scale参数, 我试了平均每个层的信息和学出来$$s_j$$发现学习出来的效果会好很多.

文章中提到的Pre-trained的language model是用了两层的biLM, 对token进行上下文无关的编码是通过CNN对字符进行编码, 然后将三层的输出scale到1024维, 最后对每个token输出3个1024维的向量表示.

### 通过AllenNLP使用ELMo

与训练的ELMo已经放出, pytorch用户可以通过AlenNLP使用, 预训的Tensorflow版本也在 [TF版](https://github.com/allenai/bilm-tf) 中放出, 这里介绍一下通过AllenNLP包用ELMo的方法.

通过pip install allennlp 或其他方法安装AllenNLP包之后, 一般我们直接调用allennlp的allennlp.commands.elmo.ElmoEmbedder 中的batch_to_embeddings对一个batch的token序列进行编码, 下面是样例做法, 第一次运行加载模型时运行时间会很长, 需要等待. 这里展示的用法不会对language model的参数进行更新, 如果需要请自己设置, 因为没有给通用接口, 所以需要修改allennlp的源码, 但是因为设置梯度回传之后效率大减, 且所需内存暴增, 我也只测试了一下, 并没有在我自己的模型中使用.

```
from allennlp.commands.elmo import ElmoEmbedder
elmo = ElmoEmbedder(options_file='../data/elmo_options.json', weight_file='../data/elmo_weights.hdf5', cuda_device=0)
context_tokens = [['I', 'love', 'you', '.'], ['Sorry', ',', 'I', 'don', "'t", 'love', 'you', '.']]
elmo_embedding, elmo_mask = elmo.batch_to_embeddings(context_tokens)
print(elmo_embedding)
print(elmo_mask)

1. 导入ElmoEmbedder类
2. 实例化ElmoEmbedder. 3个参数分别为参数配置文件, 预训练的权值文件, 想要用的gpu编号, 这里两个文件我是直接下载好的, 如果指定系统默认自动下载会花费一定的时间, 下载地址
   
    DEFAULT_OPTIONS_FILE = "https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json"
    DEFAULT_WEIGHT_FILE = "https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5"
    
3. 输入是一个list的token序列, 其中外层list的size即内层list的个数就是我们平时说的batch_size, 内层每个list包含一个你想要处理的序列(这里是一句话, 你可以一篇文章或输入任意的序列, 因为这里预训练的模型是在英文wikipidia上训的, 所以输入非英文的序列肯定得到的结果没什么意义).
4. 通过batch_to_embeddings对输入进行计算的到tokens的embedding结果以及我们输入的batch的mask信息(自动求mask)

    Variable containing:
    ( 0  , 0  ,.,.) = 
      0.6923 -0.3261  0.2283  ...   0.1757  0.2660 -0.1013
     -0.7348 -0.0965 -0.1411  ...  -0.3411  0.3681  0.5445
      0.3645 -0.1415 -0.0662  ...   0.1163  0.1783 -0.7290
               ...             ⋱             ...          
      0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000
      0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000
      0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000

    ( 0  , 1  ,.,.) = 
     -1.1051 -0.4092 -0.4365  ...  -0.6326  0.4735 -0.2577
      0.0899 -0.4828 -0.5596  ...   0.4372  0.3840 -0.7343
     -0.5538 -0.1473 -0.2441  ...   0.2551  0.0873  0.2774
               ...             ⋱             ...          
      0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000
      0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000
      0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000

    ( 0  , 2  ,.,.) = 
     -3.2634 -0.9448 -0.3199  ...  -1.2070  0.6930 -0.2016
     -0.3688 -0.7632 -0.0715  ...   0.6294  1.6869 -0.6655
     -1.0870 -1.4243 -0.2445  ...   0.0825  0.5020  0.2765
               ...             ⋱             ...          
      0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000
      0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000
      0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000
            ⋮  

    ( 1  , 0  ,.,.) = 
      0.5042 -0.6629 -0.0231  ...  -0.3084 -0.9741 -0.7230
      0.1131  0.1575  0.1414  ...   0.3718 -0.1432 -0.0248
      0.6923 -0.3261  0.2283  ...   0.1757  0.2660 -0.1013
               ...             ⋱             ...          
     -0.7348 -0.0965 -0.1411  ...  -0.3411  0.3681  0.5445
      0.3645 -0.1415 -0.0662  ...   0.1163  0.1783 -0.7290
     -0.8872 -0.2004 -1.0601  ...  -0.2655  0.2115  0.1977

    ( 1  , 1  ,.,.) = 
      0.1221 -0.7032  0.0169  ...  -0.3249 -0.4935 -0.4965
      0.3399 -0.4682  0.1888  ...  -0.0565  0.1001 -0.0416
     -0.8135 -0.8491 -0.3264  ...  -0.5674  0.2638  0.2006
               ...             ⋱             ...          
      0.4460 -0.4475 -0.1583  ...   0.4372  0.3840 -0.7343
     -0.1287  0.0161  0.0315  ...   0.2551  0.0873  0.2774
     -1.2373 -0.3373  0.1098  ...  -0.0276 -0.0181  0.0602

    ( 1  , 2  ,.,.) = 
     -0.0830 -1.5891 -0.2576  ...  -1.2944  0.1082  0.6745
     -0.0724 -0.7200  0.1463  ...   0.6919  0.9144 -0.1260
     -2.3460 -1.1714 -0.7065  ...  -1.2885  0.4679  0.3800
               ...             ⋱             ...          
      0.1246 -0.6929  0.6330  ...   0.6294  1.6869 -0.6655
     -0.5757 -1.0845  0.5794  ...   0.0825  0.5020  0.2765
     -1.2392 -0.6155 -0.9032  ...   0.0524 -0.0852  0.0805
    [torch.cuda.FloatTensor of size 2x3x8x1024 (GPU 0)]

    Variable containing:
        1     1     1     1     0     0     0     0
        1     1     1     1     1     1     1     1
    [torch.cuda.LongTensor of size 2x8 (GPU 0)]

输出两个Variable, 第一个是2*3*8*1024的embedding信息, 第二个是mask, 其中2是batch_size, 3是两层biLM的输出加一层CNN对character编码的输出, 8是最长list的长度(对齐), 1024是每层输出的维度; mask的输出2是batch_size, 8实在最长list的长度, 第一个list有4个tokens, 第二个list有8个tokens, 所以对应位置输出1.

```

### 结语

ELMo的效果非常好, 我自己在SQuAD数据集上可以提高3个左右百分点的准确率. 因为是上下文相关的embedding, 所以在一定程度上解决了一词多义的语义问题. 

但是ELMo速度非常慢, 因为对每个token编码都要通过language model计算的出, 不如之前fix的embedding直接拿来用, 效率低到令人发指, 没有充足的计算资源会很难受. 这里一个解决办法是, 我们一般对模型需要多轮的训练, 每次训练都会重新通过language model计算token, 而我们不进行梯度回传更新biLM的参数, 所以我们输入相同的句子(文章或其他序列)输出结果不会改变, 因此我们可以只在第一个epoch中通过biLM计算token的表示, 然后我们保存起来, 下一次用到这个序列时直接加载, 可以节省大量时间.

