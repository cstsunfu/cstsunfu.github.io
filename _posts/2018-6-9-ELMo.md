---
layout: post
title: 一种新的word embedding --Deep contextualized word representations (ELMo)
date: 2018-06-09
tags: Paper
---

### Introduction

之前的glove以及word2vec的word embedding在nlp任务中都取得了最好的效果, 现在几乎没有一个NLP的任务中不加word embedding.

我们常用的获取embedding方法都是通过训练language model, 将language model中预测的hidden state做为word的表示, 给定N个tokens的序列$$(t_1, t_2,...,t_n)$$, 前向language model就是通过前k-1个输入序列$$(t_1, t_2, ...,t_k)$$的hidden表示, 预测第k个位置的token, 反向的language model就是给定后面的序列, 预测之前的, 然后将language model的第k个位置的hidden输出做为word embedding. 

之前的做法的缺点是对于每一个单词都有唯一的一个embedding表示, 而对于多义词显然这种做法不符合直觉, 而单词的意思又和上下文相关, ELMo的做法是我们只预训练language model, 而word embedding是通过输入的句子实时输出的, 这样单词的意思就是上下文相关的了, 这样就很大程度上缓解了歧义的发生.

### ELMo: Embeddings from Language Models

ELMo用到上文提到的双向的language model, 给定N个tokens (t1, t2,...,tN),  language model通过给定前面的k-1个位置的token序列计算第k个token的出现的概率:

$$p(t_1, t_2, ..., t_N) = \prod_{k=1}^N p(t_k|t_1, t_2, ..., t_{k-1})$$

后向的计算方法与前向相似:

$$p(t_1, t_2, ..., t_N) = \prod_{k=1}^N p(t_k|t_{k+1}, t_{k+2}, ..., t_{N})$$

biLM训练过程中的目标就是最大化:

$$\sum^N_{k=1}(\log p(t_k| t_1, ...,t_{k-1};\Theta _x, \overrightarrow{\Theta}_{LSTM}, \Theta _s) + \log p(t_k| t_{k+1}, ...,t_{N}; \Theta _x, \overleftarrow{\Theta}_{LSTM}, \Theta _s))$$
