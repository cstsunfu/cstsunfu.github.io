---
layout: post
title: Reinforce learning 笔记(1)
date: 2017-11-26
category: 
tags: Reinforce learning
---

### Introduction of Reinforce Learning

强化学习算法通过特定策略执行一个动作，执行之后通过执行结果的反馈修改算法的策略，然后迭代，最终学到最优的策略。

下面先给出几个定义：

给定一个策略 $\pi$ ,定义如下：
$$\pi(a|s)=p[A_t=a|S_t=s]$$
表示在s状态下执行动作a的概率分布

当给定策略 $\pi$ 时，回报为：
$$G_t=R_{t+1}+\gamma R_{t+2}+...=\sum_{k=0}^\infty \gamma ^k R_{t+k+1}$$
其中 $\gamma$ 是衰变率，很好理解，将来的奖励的重要程度对于现在来说肯定是越近越好，就好像你是愿意做一件几天之后就有丰厚回报的工作，还是十年之后才能有回报的事啊（这也很好的解释了为什么都不愿意读博士）。

状态值函数，即某个状态s的价值估值的期望
$$v_{\pi}(s)=E_{\pi}[G_t|S_{t}=s]$$
状态行为值函数,即在状态s下执行action a的价值：
$$q_{\pi}(s,a) = E_{\pi}[G_t|S_t = s,A_t = a]$$

### 优化策略

强化学习的动态规划方程：

$$v^{\*}(s)=\max_aR^a_s + \gamma \sum _{s'\in{S}} P_{ss'}^{a} max_{a'} q^{\*}(s',a') $$

