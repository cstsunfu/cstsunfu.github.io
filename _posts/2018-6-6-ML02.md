---
layout: post
title: 机器学习-决策树
date: 2018-06-06
tags: Machine Learning   
---

### 决策树基本算法

我们要判断一个复杂的问题往往通过多次简单判断来得出结论. 比如我们看到一个小姐姐我们会判断这个小姐姐是不是一个美女, 这是一个很简单的例子, 假设我们只有两个答案, 一个是,一个否, 我们首先看五官是否端正, 如果是, 我们再看有没有麻子痘痘, 如果没有我们再看看身材.. 通过上诉一系列询问最后得出结论这个小姐姐是\不是美女. 对于其中的每次判断, 都是一个决策过程, 因为我们的判断比较简单, 只会分出两种情况, 最后得到一颗二叉树, 二叉树的每个非叶子节点都代表一次决策, 每个叶子节点即我们的决策结果(论), 当然你的每次决策可能会有多种结果, 这样就会生成一个普通的树, 这棵树就是决策树.

算法:

```
输入: 训练集 D={(x1, y1), (x2, y2), ..., (xm, ym)}
      属性集 A={a1, a2, ..., ad}
过程: 函数TreeGenerate(D, A)

1:  生成节点node;
2:  if D中样本全属于同一个类别C then
        将node标记为C类叶节点; return
    end if
3:  if A 为空 OR D中样本在A上取值相同 then
        将node标记为叶节点, 其类别为D中样本数最多的类; return
    end if
4:  从A中选择最优化分属性a';
5:  for a' 中的每一个值aa do
        为node生成一个分支; 另Dv 表示D中在a'上取值为aa的样本子集;
        if Dv为空 then
            将分支节点标记为叶节点, 其类别标记为D中样本最多的类; return
        else
            以TreeGenerate(Dv, A\{a'})为分支节点
        end if
    end for
输出: 以node为根的决策树
```

上面的算法即递归建树的过程, 非常清晰, 其中只有第4步从A中选择最优化分属性a'我们不清楚什么是最优, 不同的最优化分方法效率和结果都有差别, 下面介绍几种划分方法.

### ID3( Iterative Dichotomiser )决策树

在介绍ID3算法之前先来介绍一下信息熵和信息增益的概念:

*信息熵*: 度量样本集合纯度的常用指标, 当前样本集合D中第k类样本所占比例为$$p_k$$, D的信息熵定义为 $$Ent(D) = -\sum_{k=1}^{\|y\|}p_k\log _2p_k$$.

如果我们用属性a来对D进行划分, a的取值为$$\{a^1, a^2,...,a^V\}$$, D划分之后在属性a上取值为$$a^v$$的集合为$$D^v$$, 我们计算划分之后各个集合的信息熵, 对于每个子集合我们加一个权重$$\|D^v\|/\|D\|$$

*信息增益*:

$$Gain(D, a) = Ent(D) - \sum_{v=1}^V \frac{\|D^v\|}{\|D\|} Ent(D^v)$$

一般来说, 信息增益越大获得的纯度提升也越大, ID3算法选取使得信息增益最大的属性a来划分数据集D, 即选取 $$a' = \arg \max_{a\in A} Gain(D, a)$$.

### C4.5决策树算法

ID3算法倾向于选取取值数目多的属性做为划分属性, 比如我们对所有样本都进行编号, 而编号也做为一个划分属性, 这个属性的取值数目显然等于样本总数N, 这样我们划分出N个子集合, 每个子集合中只有一个元素,每个子集合的信息熵显然为0, 所有子集合的信息熵之和也为0, 信息增益获得最大值即原数据集合的信息熵. 但是这样的做法对于算法泛化很不利, 因此提出的C4.5是以增益率做为属性选取标准的算法.

*增益率*:

$$Gain_ratio(D, a) = \frac{Gain(D,a)}{IV(a)}$$

其中

$$IV(a) = -\sum _{v=1}^V \frac{\|D^v\|}{D} \log_2 \frac{\|D^v\|}{\|D\|} $$ 

IV(a)的计算方法是不是很熟悉, 感觉不到回头看看信息熵的定义. 所以上面式子的直观理解就是, 我们希望信息增益越大越好, 但是用于划分的属性a的作用于数据集D的划分结果的信息熵越小越好(这个不好解释, 大约等价于划分出的分支集合越少越好, 当然不完全等价, 需要自己体会).

### CART决策树(Classification and Regression Tree)

CART使用基尼指数来选择划分属性.

$$Gini(D) = \sum _{k=1}^{\|y\|}\sum_{k\neq k} p_kp_{k'} = 1- \sum _{k=1}^{\|y\|}p_k^2$$

Gini表示直观理解就是D中热议两个任意属于一个类别的概率越高, Gini值越小, 因此Gini值可以做为集合纯度的度量, 我们希望划分之后的集合的纯度越高越好, 对于属性a其Gini值为

$$Gini_index(D,a)=\sum _{v=1}^V \frac{\|D^v\|}{\|D\|} Gini(D^v)$$

欲获得最高的纯度, 我们需要Gini值越小越好, 即:

$$a' = \arg \min_{a\in A} Gini_index(D, a)$$
