---
layout: post
title: 机器学习-线性模型
date: 2018-05-21
tags: Machine Learning   
---
### 线性回归

给定数据集$$D=\left\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\right\}$$, 其中$$x_i=(x_{i1};x_{i2};...;x_{id}), y_i \in \mathbb{R}$$.

最简单的情况, 当x的属性只有一个的时候, 线性回归模型学得一个线性模型以尽可能准确的预测真实值输出. 即

$$f(x_i)=wx_i+b, f(x_i)\simeq y_i$$

其中w, b为待学习的参数,通过最小化均方误差确定w和b的值:

$$(w^*,b^*) = \arg\min_{(w,b)}\sum_{i=1}^m(f(x_i)-y_i)^2=\arg\min_{(w,b)}\sum_{i=1}^m(y_i-wx_i-b)^2$$

求上式极值直接对w, b求偏导得到:

$$\frac{\partial E_{(w,b)}}{\partial w}=2\left(w\sum_{i=1}^m x_i^2-\sum_{i=1}^m(y_i-b)x_i\right)$$

$$\frac{\partial E_{(w,b)}}{\partial b}=2\left(mb-\sum_{i=1}^m(y_i-wx_i)\right)$$

另上式都等于0, 可求得闭式解:

$$w=\frac{\sum_{i=1}^m y_i(x_i-\bar x)}{\sum_{i=1}^m x_i^2 - \frac{1}{m}\left(\sum_{i=1}^m x_i\right)^2}$$

$$b = \frac{1}{m}\sum_{i=1}^m(y_i-wx_i)$$

通常情况下, x有d个属性, 此时

$$
\left[
\begin{matrix}
x_1^T & 1\\
x_2^T & 1\\
\vdots & \vdots\\
x_m^T & 1\\
\end{matrix}
\right]
$$

其中$$x_i^T = (x_{i1}, x_{i2}, \cdots, x_{id})$$, 有

$$ w^* = \arg\min_{w} (y-Xw)^T(y-Xw) $$

对w求导得 $$2X^T(Xw-y)$$, 另其为0, 解得闭式解 $$w=(X^TX)^{-1}X^Ty$$, 此时得到的现行回归模型为$$f(x_i) = x_i^T(X^TX)^{-1}X^Ty$$, 这里有一步求逆的过程, 而对于不是满秩的情况矩阵没有逆矩阵, 此时计算其伪逆.

而当y与x不具有线性关系时, 如果我们可以找到一个函数g(y)使得 $$g(y) = w^Tx+b$$, 同样可以通过上面的方法进行计算.

### 分类

对于二分类问题, 只需要用上面求解回归问题的方法对函数通过一个阈值来使函数大于该值时为一个类, 小于时为另一个类, 不过通常我们将函数值过一个sigmoid函数来分类, 这样不知可以用阈值来分类还有概率信息即sigmoid输出的结果可以做为分类的概率值.

对于N分类问题, 可以将问题转换为N个二分类问题, 第k个二分类器判断输入是否为第k个类, 还可以对任意两个类训练一个二分类器, 不过这样需要N*(N-1)/2个2分类器, 一般来说较复杂.

### 类别不平衡问题

一般我们求一个二分类问题, sigmoid输出的结果通过$$\frac{y}{1-y} > 1$$ 预测为正例.

当正例负例数目不同时, 可以通过再缩放来进行预测$$\frac{y^\prime}{1-y^\prime}=\frac{y}{1-y}\frac{m^-}{m^+}$$, 其中$$m^-$$为观测的负例数, $$m^+$$为正例数.

然而有时候我们的观测正负例比例和真实比例并不相同, 可以通过对负例欠采样, 对正例进行过采样, 或者将负例分成n等份, 每一份样例数和正例相同, 这三种方法各有优缺点, 其中对正例过采样实现困难, 对负例欠采样在训练集不够大的时候也不适合.

### 线性判别分析(Linear Discriminant Analysis) LDA

LDA的思想是将训练数据通过线性变换映射到另一个空间, 在这个空间中类别之间的差距越大越好, 类别内部差距越小越好.

给定数据集$$D=\{(x_i, y_i)\}_{i=1}^m, y_i \in \{0,1\}$$, $$X_i, \mu_i, \Sigma_i$$分别表示第i类的集合, 均值, 协方差矩阵.

如果将其映射到w上, 此时第i个类别的中心点(均值位置)被映射到$$w\mu_i$$. 第i个样本的协方差为$$w^T\Sigma_iw$$.

对于只有两类的问题来说, 我们希望类别间中心点距离越远越好, 即$$\lVert w\mu_0 - w\mu_1 \rVert _2^2$$取最大. 类内差别越小越好, 即类内方差越小越好, $$w^T\Sigma_0w + w^T\Sigma_1w$$取最小. 两个合起来优化即最大化:

$$J= \frac{\lVert w\mu_0 - w\mu_1 \rVert _2^2}{w^T\Sigma_0w + w^T\Sigma_1w}=\frac{w^T(\mu_0 - \mu_1)(\mu_0 - \mu_1)^Tw}{w^T(\Sigma_0 + \Sigma_1)w}$$ 

_定义_:

*类内散度矩阵*: $$S_w=\Sigma_0+\Sigma_1 = \sum_{x\in X_0}(x-\mu_0)(x-\mu_0)^T+\sum_{x\in X_1}(x-\mu_1)(x-\mu_1)^T$$

*类间散度矩阵*: $$S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^T$$

*全局散度矩阵*: $$S_t = S_b + S_w = \sum_{i=1}^m(x_i-\mu)(x_i-\mu)^T$$, 其中$$\mu$$是所有数据的均值

我们重写优化目标为:$$J= \frac{w^TS_bw}{w^TS_ww} $$

我们要找到一个w使得上式最大, 上式中分子和分母都是w的二次项, 因此我们可以将w任意缩放而上式结果不变, 因此我们不妨另$$w^TS_ww=1$$, 于是有:

$$\min_w -w^TS_bw$$

$$\text{subject to: } w^TS_ww=1$$

由拉格朗日乘子法上式等价于: $$S_bw=\lambda S_ww$$, 将$$S_b$$的表达式带入其中$$(\mu_0-\mu_1)w$$为一个常数, 所以有$$w=S_w^{-1}(\mu_0-\mu_1)$$.

通常对$$S_w$$求逆通过先进行奇异值分解得到$$U\Sigma V^T$$ 其中$$\Sigma$$为实对角矩阵, $$S_w^{-1} = V\Sigma^{-1}U^T$$得到, 简化运算.

多分类问题原理相同, 不赘述.


LDA可以做为监督降维的方法, 其中监督信号就是数据的类别, 可以将数据映射到维度较低的w平面.
