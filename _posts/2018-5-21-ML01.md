---
layout: post
title: 机器学习-线性模型
date: 2018-05-21
tags: Machine Learning   
---
### 线性回归

给定数据集$$D=\left\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\right\}$$, 其中$$x_i=(x_{i1};x_{i2};...;x_{id}), y_i \in \mathbb{R}$$.

最简单的情况, 当x的属性只有一个的时候, 线性回归模型学得一个现行模型以尽可能准确的预测真实值输出. 即

$$f(x_i)=wx_i+b, f(x_i)\simeq y_i$$

其中w, b为待学习的参数,通过最小化均方误差确定w和b的值:

$$(w^*,b^*) = \arg\min_{(w,b)}\sum_{i=1}^m(f(x_i)-y_i)^2=\arg\min_{(w,b)}\sum_{i=1}^m(y_i-wx_i-b)^2$$

求上式极值直接对w, b求偏导得到:

$$\frac{\partial E_{(w,b)}}{\partial w}=2\left(w\sum_{i=1}^m x_i^2-\sum_{i=1}^m(y_i-b)x_i\right)$$

$$\frac{\partial E_{(w,b)}}{\partial b}=2\left(mb-\sum_{i=1}^m(y_i-wx_i)\right)$$

另上式都等于0, 可求得闭式解:

$$w=\frac{\sum_{i=1}^m y_i(x_i-\bar x)}{\sum_{i=1}^m x_i^2 - \frac{1}{m}\left(\sum_{i=1}^m x_i\right)^2}$$

$$b = \frac{1}{m}\sum_{i=1}^m(y_i-wx_i)$$

通常情况下, x有d个属性, 此时

$$
\left[
\begin{matrix}
x_1^T & 1\\
x_2^T & 1\\
\vdots & \vdots\\
x_m^T & 1\\
\end{matrix}
\right]
$$

其中$$x_i^T = (x_{i1}, x_{i2}, \cdots, x_{id})$$, 有

$$ w^* = \arg\min_{w} (y-Xw)^T(y-Xw) $$

对w求导得 $$2X^T(Xw-y)$$, 另其为0, 解得闭式解 $$w=(X^TX)^{-1}X^Ty$$, 此时得到的现行回归模型为$$f(x_i) = x_i^T(X^TX)^{-1}X^Ty$$, 这里有一步求逆的过程, 而对于不是满秩的情况矩阵没有逆矩阵, 此时计算其伪逆.

而当y与x不具有线性关系时, 如果我们可以找到一个函数g(y)使得 $$g(y) = w^Tx+b$$, 同样可以通过上面的方法进行计算.

### 分类

对于二分类问题, 只需要用上面求解回归问题的方法对函数通过一个阈值来使函数大于该值时为一个类, 小于时为另一个类, 不过通常我们将函数值过一个sigmoid函数来分类, 这样不知可以用阈值来分类还有概率信息即sigmoid输出的结果可以做为分类的概率值.

对于N分类问题, 可以将问题转换为N个二分类问题, 第k个二分类器判断输入是否为第k个类, 还可以对任意两个类训练一个二分类器, 不过这样需要N*(N-1)/2个2分类器, 一般来说较复杂.

### 类别不平衡问题

一般我们求一个二分类问题, sigmoid输出的结果通过$$\frac{y}{1-y} > 1$$ 预测为正例.

当正例负例数目不同时, 可以通过再缩放来进行预测$$\frac{y^\prime}{1-y^\prime}=\frac{y}{1-y}\frac{m^-}{m^+}$$, 其中$$m^-$$为观测的负例数, $$m^+$$为正例数.

然而有时候我们的观测正负例比例和真实比例并不相同, 可以通过对负例欠采样, 对正例进行过采样, 或者将负例分成n等份, 每一份样例数和正例相同, 这三种方法各有优缺点, 其中对正例过采样实现困难, 对负例欠采样在训练集不够大的时候也不适合.
